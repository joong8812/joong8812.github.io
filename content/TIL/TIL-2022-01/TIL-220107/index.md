---
emoji: ğŸ§ 
title: Today I Learned - 220107
date: '2022-01-07 18:00:00'
author: ì½”ë”©ì¿ ë‹ˆ
tags: TIL AI DeepLearning Colab
categories: TIL AI
---

# ë¨¸ì‹ ëŸ¬ë‹ 3ì£¼ì°¨ Homework
___

> **â“** ë¬¸ì œ
>
> ì†ìœ¼ë¡œ ì“´ [0~9 ìˆ«ì ì´ë¯¸ì§€ ë°ì´í„°ì…‹](https://www.kaggle.com/oddrationale/mnist-in-csv)ìœ¼ë¡œ ë”¥ëŸ¬ë‹(Depp Laerning) ëª¨ë¸ êµ¬í˜„í•˜ê¸° 

## 1. í•„ìš”í•œ ë°ì´í„°ì…‹ ë‹¤ìš´ ë° íŒ¨í‚¤ì§€ import
```python
import os
os.environ['KAGGLE_USERNAME'] = 'username'
os.environ['KAGGLE_KEY'] = 'key' 
```
```python
!kaggle datasets download -d oddrationale/mnist-in-csv
```
```python
!unzip mnist-in-csv.zip
```
```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam, SGD
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
```

## 2. íŠ¸ë ˆì´ë‹ & ê²€ì¦ ë°ì´í„°ì…‹ Dataframe ë§Œë“¤ì–´ì£¼ê¸°
```python
train_df = pd.read_csv('mnist_train.csv')
train_df.head() # ë°ì´í„° 5í–‰ê¹Œì§€ í™•ì¸
```
```python
test_df = pd.read_csv('mnist_test.csv')
test_df.head()# ë°ì´í„° 5í–‰ê¹Œì§€ í™•ì¸
```

## 3. íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ Output ê²°ê³¼ê°’(ìˆ«ìë³„) ê°¯ìˆ˜ í™•ì¸
```python
plt.figure(figsize=(16, 10))
sns.countplot(train_df['label'])
plt.show()
```
![dataset](./output_countplot.png)

## 4. íŠ¸ë ˆì´ë‹ & ê²€ì¦ ë°ì´í„°ì…‹ ì…ë ¥ê°’ê³¼ ì¶œë ¥ê°’ ë¶„ë¦¬
```python
train_df = train_df.astype(np.float32)
x_train = train_df.drop(columns=['label'], axis=1).values
y_train = train_df[['label']].values

test_df = test_df.astype(np.float32)
x_test = test_df.drop(columns=['label'], axis=1).values
y_test = test_df[['label']].values

print(x_train.shape, y_train.shape) # (60000, 784) (60000, 1) 6ë§Œê°œì˜ íŠ¸ë ˆì´ë‹ë°ì´í„°ì…‹
print(x_test.shape, y_test.shape) # (10000, 784) (10000, 1) 1ë§Œê°œì˜ ê²€ì¦ë°ì´í„°ì…‹
```

## 5. íŠ¸ë ˆì´ë‹ì…‹ì˜ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ì¶œë ¥ í•´ë³´ì
ì´ê±° ë„ˆë¬´ ì‹ ê¸°í•˜ë‹¤! í•œ ë°ì´í„°ì”© 784ê°œ(28 x 28)ì˜ í”½ì…€ ì»¬ëŸ¬ê°’ì„ ê°€ì§€ê³  ìˆë‹¤. ìš”ê±¸ matplotlib ì„ ì´ìš©í•´ ê·¸ë ¤ì£¼ë©´ ìœ„ì™€ ê°™ì´ ìˆ«ìê°€ ë³´ì¸ë‹¤. ì¦‰, ìˆ«ì ì´ë¯¸ì§€ -> ê° í”½ì…€ì˜ ì»¬ëŸ¬ ê°’ìœ¼ë¡œ ë°ì´í„°í™”. ì´ë¯¸ì§€ë¥¼ ë°ì´í„°í™” í•œ ê²ƒë„ ë˜ ê·¸ ë°ì´í„°ë¥¼ ê°„ë‹¨í•œ í•¨ìˆ˜ë¡œ ì´ë¯¸ì§€ë¥¼ ë³¼ìˆ˜ ìˆëŠ” ê²Œ ì •ë§ ì‹ ê¸°í•˜ë‹¤.
```python
index = 3
plt.title(str(y_train[index]))
plt.imshow(x_train[index].reshape((28, 28)), cmap='gray')
plt.show()
```
![check_imagedata](./check_imagedata.png)

## 6. ì¶œë ¥ê°’(0~9)ì„ One-hot encoding ë°ì´í„°ë¡œ ë³€í™˜
ì»´í“¨í„°ê°€ ì¢‹ì•„í•˜ëŠ” 0ê³¼1ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°ë¡œ ë°ì´í„°ë¥¼ êµ¬ë³„í•´ì¤€ë‹¤.   
0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   
1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   
(ìƒëµ)   
8 -> [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]   
9 -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 1]
```python
encoder = OneHotEncoder()
y_train = encoder.fit_transform(y_train).toarray()
y_test = encoder.fit_transform(y_test).toarray()
```

## 7. ì…ë ¥ê°’ì„ ì •ê·œí™”(Normalization) í•œë‹¤.
ì´ë¯¸ì§€ ë°ì´í„°ëŠ” í”½ì…€ì´ 0-255(uint8)ë¡œ ë˜ì–´ ìˆìŒ. 255ë¡œ ë‚˜ëˆ„ì–´ 0-1 ì‚¬ì´ì˜ ì†Œìˆ˜ì  ë°ì´í„°(float32)ë¡œ ë°”ê¾¸ê³  ì¼ë°˜í™”
```python
x_train = x_train / 255.
x_test = x_test / 255.
```

## 8. ëª¨ë¸ êµ¬í˜„
```python
input = Input(shape=(784,)) # 784ê°œì˜ input
hidden = Dense(1024, activation='relu')(input) # 1024 hidden layers + relu(activation function)
hidden = Dense(512, activation='relu')(hidden) # 512 hidden layers + relu(activation function)
hidden = Dense(256, activation='relu')(hidden) # 256 hidden layers + relu(activation function)
output = Dense(10, activation='softmax')(hidden) # 10ê°œì˜ output + softmax

model = Model(inputs=input, outputs=output)

model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])

model.summary()
```
![model_summary](./model_summary.png)

## 9. í•™ìŠµ
ë°ì´í„°ì™€ í•™ìŠµëŸ‰ì´ ë§ê¸° ë•Œë¬¸ì— colab GPUë¥¼ ì¼œì!
```python
history = model.fit(
    x_train,
    y_train,
    validation_data=(x_test, y_test), # ê²€ì¦ ë°ì´í„°ë¥¼ ë„£ì–´ì£¼ë©´ í•œ epochì´ ëë‚ ë•Œë§ˆë‹¤ ìë™ìœ¼ë¡œ ê²€ì¦
    epochs=20 # epochs ë³µìˆ˜í˜•ìœ¼ë¡œ ì“°ê¸°!
)
```

## 10. í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„
* íŒŒë€ìƒ‰: íŠ¸ë ˆì´ë‹ë°ì´í„°ì…‹ ë¡œìŠ¤ / ì£¼í™©ìƒ‰: ê²€ì¦ë°ì´í„°ì…‹ ë¡œìŠ¤
* ê²€ì¦ ë°ì´í„°ì…‹ì˜ ë¡œìŠ¤ëŠ” 0ìœ¼ë¡œ ê°€ë‹¤ê°€ ì ì  ìƒìŠ¹ ê³¡ì„ ìœ¼ë¡œ ê·¸ë¦¬ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ `overfitting`ì˜ ëª¨ìŠµì„ ë¤ë‹¤.
```python
plt.figure(figsize=(16, 10))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
```
![loss](./output_loss.png)
* íŒŒë€ìƒ‰: íŠ¸ë ˆì´ë‹ë°ì´í„°ì…‹ ì •í™•ë„ / ì£¼í™©ìƒ‰: ê²€ì¦ë°ì´í„°ì…‹ ì •í™•ë„
* ê±°ì˜ 100ì— ê°€ê¹Œìš´ êµ‰ì¥íˆ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤
```python
plt.figure(figsize=(16, 10))
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
```
![accuracy](./output_accuracy.png)

## ì´í‰
* ë§ë¡œë§Œ ë“£ë˜ tensorflow, pandas, numpy ë“±ì˜ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•´ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ë‹¤ë‹ˆ ì¬ë¯¸ìˆëŠ” ìˆ™ì œì˜€ë‹¤:)
* ìœ„ì™€ ê°™ì€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ epochì´ 20 ì •ë„ì¸ë°ë„ ì‹œê°„ì´ ìƒë‹¹íˆ ì†Œìš”ëœë‹¤. í•œ epochë‹¹ 10ì´ˆ ì •ë„ ê±¸ë¦¬ëŠ” ê²ƒ ê°™ë‹¤.
* ì´ë¡ ì— ëŒ€í•´ì„œëŠ” ê·¸ë ‡ê²Œ ê¹Šê²Œ ì•Œê³  ìˆì§€ ëª»í•˜ëŠ”ë°, ì´ ì •ë„ë§Œ ì•Œì•„ë„ ë˜ëŠ”ê±´ì§€ ëª¨ë¥´ê² ë‹¤.
* ë‹¤ìŒì£¼ ìƒˆí”„ë¡œì íŠ¸ ë“¤ì–´ê°ˆ ë•Œ ì‚¬ë¬¼ì¸ì‹ ê¸°ìˆ ì„ ì ìš©í•œ í”„ë¡œì íŠ¸ë¼ëŠ” ì´ì•¼ê¸°ê°€ ìˆëŠ”ë°, ì–´ë–¤ í”„ë¡œì íŠ¸ì¼ì§€ ê°ì´ ì•ˆ ì˜¨ë‹¤.
```toc
```